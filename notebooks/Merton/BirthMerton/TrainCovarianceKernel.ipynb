{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2f12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.distributions import Normal, Poisson, MultivariateNormal\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from bdp.models.utils.basic_setups import create_dir_and_writer\n",
    "\n",
    "from bdp.models.gaussian_processes.gaussian_processes import multivariate_normal, white_noise_kernel\n",
    "\n",
    "from bdp.models.random_fields.blocks_utils_torch import calculate_determinant_and_inverse_covariance_history_torch\n",
    "from bdp.models.random_fields.blocks_utils_torch import obtain_location_index_to_realization, obtain_blocks\n",
    "from bdp.models.random_fields.blocks_utils_torch import log_probability_from_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa98e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dimension = 2\n",
    "prior_locations_mean =  0.\n",
    "prior_locations_std = 6.69\n",
    "number_of_realizations = 100\n",
    "birth_intensity = 2.\n",
    "kernel_sigma = 10.\n",
    "kernel_lenght_scales = [1., 2.]\n",
    "\n",
    "locations_prior = Normal(torch.full((locations_dimension,), prior_locations_mean),\n",
    "                         torch.full((locations_dimension,), prior_locations_std))\n",
    "\n",
    "# Births\n",
    "birth_distribution = Poisson(birth_intensity)\n",
    "birth_numbers = birth_distribution.sample((number_of_realizations,)).long()\n",
    "assets_in_the_market = birth_numbers.cumsum(dim=0)\n",
    "total_assets_in_history = assets_in_the_market[-1]\n",
    "\n",
    "# Locations\n",
    "locations_history = locations_prior.sample(sample_shape=(total_assets_in_history,))\n",
    "    \n",
    "def define_kernel(kernel_sigma, kernel_lenght_scales):\n",
    "    kernel = ScaleKernel(RBFKernel(ard_num_dims=locations_dimension, requires_grad=True),\n",
    "                         requires_grad=True) + white_noise_kernel()\n",
    "\n",
    "    kernel_hypers = {\"raw_outputscale\": torch.tensor(kernel_sigma),\n",
    "                     \"base_kernel.raw_lengthscale\": torch.tensor(kernel_lenght_scales)}\n",
    "\n",
    "    kernel.kernels[0].initialize(**kernel_hypers)\n",
    "    kernel_eval = lambda locations: kernel(locations, locations).evaluate().type(torch.float64)\n",
    "    \n",
    "    return kernel, kernel_eval\n",
    "    \n",
    "def define_covariance_matrix_from_kernel(number_of_realizations,birth_intensity,kernel_sigma,kernel_lenght_scales):    \n",
    "    number_of_realizations = number_of_realizations\n",
    "\n",
    "    # Kernel\n",
    "    kernel, kernel_eval = define_kernel(kernel_sigma,kernel_lenght_scales)\n",
    "    covariance_diffusion_history = kernel_eval(locations_history)\n",
    "    \n",
    "    return covariance_diffusion_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728ae93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.ones(total_assets_in_history).type(torch.float64)*0.01\n",
    "covariance_matrix = define_covariance_matrix_from_kernel(number_of_realizations,birth_intensity,kernel_sigma,kernel_lenght_scales)\n",
    "# generate data\n",
    "distribution = MultivariateNormal(mu,covariance_matrix)\n",
    "sample = distribution.sample(sample_shape=(number_of_realizations,))\n",
    "for i,current_number_of_assets in enumerate(assets_in_the_market):\n",
    "    sample[i,current_number_of_assets:] = torch.zeros_like(sample[i,current_number_of_assets:])\n",
    "log_returns = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4c35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "determinants_history, inverse_covariance_history = calculate_determinant_and_inverse_covariance_history_torch(\n",
    "    assets_in_the_market, covariance_matrix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4822321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real 7.47916565598642e-23 Estimated 7.479653520082225e-23\n",
      "Last Element of Diagonal 1.0000000132827986\n"
     ]
    }
   ],
   "source": [
    "print(\"Real {0} Estimated {1}\".format(torch.det(covariance_matrix),determinants_history[-1]))\n",
    "print(\"Last Element of Diagonal {0}\".format(torch.matmul(covariance_matrix,inverse_covariance_history[-1])[-1,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db501bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MertonBirthKernel(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        nn.Module.__init__(self)\n",
    "        self.locations_dimension = kwargs.get(\"locations_dimension\")\n",
    "        self.define_deep_parameters()\n",
    "        \n",
    "    def define_kernel(self):\n",
    "        kernel = ScaleKernel(RBFKernel(ard_num_dims=self.locations_dimension, requires_grad=True),\n",
    "                             requires_grad=True) + white_noise_kernel()\n",
    "\n",
    "        kernel_hypers = {\"raw_outputscale\": torch.tensor(self.kernel_sigma),\n",
    "                         \"base_kernel.raw_lengthscale\": torch.tensor(self.kernel_lenght_scales)}\n",
    "\n",
    "        kernel.kernels[0].initialize(**kernel_hypers)\n",
    "        kernel_eval = lambda locations: kernel(locations, locations).evaluate().type(torch.float64)\n",
    "\n",
    "        return kernel, kernel_eval\n",
    "\n",
    "    def forward(self, locations_history):\n",
    "        kernel, kernel_eval = define_kernel(kernel_sigma,kernel_lenght_scales)\n",
    "        covariance_diffusion_history = kernel_eval(locations_history)\n",
    "        return covariance_diffusion_history\n",
    "\n",
    "    def define_deep_parameters(self):\n",
    "        self.kernel_sigma = nn.Parameter(torch.Tensor([1.]))\n",
    "        self.kernel_lenght_scales = nn.Parameter(torch.Tensor([20.,30.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ade942a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.DoubleTensor []], which is output 0 of AsStridedBackward0, is at version 100; expected version 99 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m determinants_history, inverse_covariance_history \u001b[38;5;241m=\u001b[39m calculate_determinant_and_inverse_covariance_history_torch(\n\u001b[0;32m      8\u001b[0m     assets_in_the_market, covariance_matrix, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m log_probability_from_blocks(sample,mu,assets_in_the_market,determinants_history,inverse_covariance_history)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda4\\envs\\neural_spectral_sde\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cesar\\anaconda4\\envs\\neural_spectral_sde\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.DoubleTensor []], which is output 0 of AsStridedBackward0, is at version 100; expected version 99 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "number_of_epochs = 100\n",
    "mbk = MertonBirthKernel(locations_dimension=locations_dimension)\n",
    "mbk.train()\n",
    "optimizer = Adam(mbk.parameters(), lr=0.001)\n",
    "for i in tqdm.tqdm(range(number_of_epochs)):\n",
    "    covariance_matrix = mbk(locations_history)\n",
    "    determinants_history, inverse_covariance_history = calculate_determinant_and_inverse_covariance_history_torch(assets_in_the_market, covariance_matrix, False)\n",
    "    loss = log_probability_from_blocks(sample,mu,assets_in_the_market,determinants_history,inverse_covariance_history).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d2c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
