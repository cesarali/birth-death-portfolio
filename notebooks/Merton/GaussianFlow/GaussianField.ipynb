{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef95845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import matmul as m\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from torch.distributions import Normal, Poisson, MultivariateNormal\n",
    "\n",
    "from deep_fields import project_path\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from deep_fields.models.utils.basic_setups import create_dir_and_writer\n",
    "from deep_fields.models.gaussian_processes.gaussian_processes import multivariate_normal, white_noise_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a6a4d9",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "d\\mathbf{x}_t = \\mu(\\mathbf{x}_t)dt + \\sqrt{\\Sigma(\\mathbf{x}_t)}dW_t\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mu(\\mathbf{x}) = K_{\\mathbf{x},Z_f}K^{-1}_{Z_f Z_f}\\text{vec}(U_f) \\\\\n",
    "\\Sigma(\\mathbf{x}) = K_{\\mathbf{x}\\mathbf{x}} - K_{\\mathbf{x},Z_f}K^{-1}_{Z_f Z_f}K_{Z_f,\\mathbf{x}} \\\\\n",
    "p(U_f) = \\prod^{D}_{d=1}\\mathcal{N}(0,K_{f_d,f_d}) \\\\\n",
    "K_{Z_f Z_f} \\in \\mathbb{R}^{MD\\times MD}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ccf091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,kernel_sigma,kernel_lenght_scales,locations_dimension=1):\n",
    "        nn.Module.__init__(self)        \n",
    "        kernel = ScaleKernel(RBFKernel(ard_num_dims=locations_dimension, requires_grad=True),\n",
    "                     requires_grad=True) + white_noise_kernel()\n",
    "        kernel_hypers = {\"raw_outputscale\": torch.tensor(kernel_sigma),\n",
    "                         \"base_kernel.raw_lengthscale\": torch.tensor(kernel_lenght_scales)}\n",
    "        kernel.kernels[0].initialize(**kernel_hypers)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x).evaluate()\n",
    "        return mean_x,covar_x\n",
    "    \n",
    "    def calculate_posterior(test_input,train_output,train_input,output_beta=None):\n",
    "        \"\"\"\n",
    "        :param test_input: torch tensor\n",
    "        :param train_input: torch tensor\n",
    "        :param kernel:\n",
    "        :return: predictive_mean [number_of_points], predictive_variance [number_of_points]\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        :param test_input: torch tensor\n",
    "        :param train_input: torch tensor\n",
    "        :param kernel:\n",
    "        :return: predictive_mean [number_of_points], predictive_variance [number_of_points]\n",
    "        \"\"\"\n",
    "\n",
    "        K_train_train = self.covar_module.forward(train_input, train_input)\n",
    "        K_test_train = self.covar_module.forward(test_input, train_input)\n",
    "        K_test_test = self.covar_module.forward(test_input, test_input, diag=True)\n",
    "\n",
    "        if output_beta is None:\n",
    "            K_train_train = K_train_train.evaluate()\n",
    "        else:\n",
    "            K_train_train = K_train_train.evaluate() + torch.tensor(1./output_beta * np.eye(len(train_input)))\n",
    "\n",
    "        prior_mean = self.mean_module(train_input)\n",
    "        prior_normal = multivariate_normal(prior_mean , K_train_train)\n",
    "\n",
    "        K_train_train_inverse = prior_normal.varinv()\n",
    "        kappa = K_test_train.evaluate().matmul(K_train_train_inverse).double()\n",
    "\n",
    "        predictive_mean = kappa.matmul(train_output.double().T).double()\n",
    "\n",
    "        predictive_variance = kappa.matmul(K_test_train.evaluate().T)\n",
    "        predictive_variance = predictive_variance.diag()\n",
    "        predictive_variance = K_test_test - predictive_variance\n",
    "\n",
    "        return predictive_mean.T[0], predictive_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf1751",
   "metadata": {},
   "source": [
    "# Inducing Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103a5fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScaleKernel(\n",
       "  (base_kernel): RBFKernel(\n",
       "    (raw_lengthscale_constraint): Positive()\n",
       "  )\n",
       "  (raw_outputscale_constraint): Positive()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_dimension = 2\n",
    "kernel_parameters = {\"kernel_sigma\": .1,\n",
    "                     \"kernel_lenght_scales\": [0.1, 0.1]}\n",
    "number_of_time_steps = 50\n",
    "        \n",
    "kwargs = {\"locations_dimension\": locations_dimension,\n",
    "          \"total_assets_in_history\":100,\n",
    "          \"number_of_inducing_points\":10,\n",
    "          \"jump_size_scale_prior\": 1.,\n",
    "          \"birth_intensity\":4.,\n",
    "          \"returns_mean_a\": 1.,\n",
    "          \"returns_mean_b\": 1.,\n",
    "          \"prior_locations_mean\": 0.,\n",
    "          \"prior_locations_std\": 1.,\n",
    "          \"prior_sigma_mean\": 0.,\n",
    "          \"prior_sigma_std\": 1.,\n",
    "          \"prior_length_mean\": 0.,\n",
    "          \"prior_length_std\": 1.,\n",
    "          \"kernel_parameters\": kernel_parameters,\n",
    "          \"number_of_realizations\": number_of_time_steps,\n",
    "          \"model_path\": os.path.join(project_path, 'results')}\n",
    "                  \n",
    "locations_dimension = kwargs.get(\"locations_dimension\")\n",
    "prior_locations_mean = kwargs.get(\"prior_locations_mean\")\n",
    "prior_locations_std = kwargs.get(\"prior_locations_std\")\n",
    "\n",
    "kernel_sigma = kernel_parameters.get(\"kernel_sigma\")\n",
    "kernel_lenght_scales = kernel_parameters.get(\"kernel_lenght_scales\")\n",
    "\n",
    "total_assets_in_history = kwargs.get(\"total_assets_in_history\")\n",
    "number_of_inducing_points = kwargs.get(\"number_of_inducing_points\")\n",
    "\n",
    "# Expected Returns\n",
    "returns_mean_a = kwargs.get(\"returns_mean_a\")\n",
    "returns_mean_b = kwargs.get(\"returns_mean_b\")\n",
    "\n",
    "# Locations Prior\n",
    "locations_prior = Normal(torch.full((locations_dimension,), prior_locations_mean),\n",
    "                         torch.full((locations_dimension,), prior_locations_std))\n",
    "\n",
    "# Mean Function\n",
    "mean_module = gpytorch.means.ZeroMean()\n",
    "\n",
    "# Kernels Prior\n",
    "kernel = ScaleKernel(RBFKernel(ard_num_dims=locations_dimension, requires_grad=True),\n",
    "             requires_grad=True) + white_noise_kernel()\n",
    "kernel_hypers = {\"raw_outputscale\": torch.tensor(kernel_sigma),\n",
    "                 \"base_kernel.raw_lengthscale\": torch.tensor(kernel_lenght_scales)}\n",
    "kernel.kernels[0].initialize(**kernel_hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2b4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_total = locations_prior.sample(sample_shape=(total_assets_in_history,))\n",
    "locations_inducing_points = locations_prior.sample(sample_shape=(number_of_inducing_points,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfdd5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_inducing_inducing = kernel.forward(locations_inducing_points, locations_inducing_points)\n",
    "K_total_inducing = kernel.forward(locations_total, locations_inducing_points)\n",
    "K_total_total = kernel.forward(locations_total, locations_total)\n",
    "\n",
    "K_total_total = K_total_total.evaluate()\n",
    "K_total_inducing = K_total_inducing.evaluate()\n",
    "K_inducing_inducing = K_inducing_inducing.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f994521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inducing Function\n",
    "inducing_mean = mean_module(train_input)\n",
    "inducing_distribution = multivariate_normal(inducing_mean , K_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27f9ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_inducing_inducing_inverse = inducing_distribution.varinv()\n",
    "kappa = K_total_inducing.matmul(K_inducing_inducing_inverse)\n",
    "total_variance = kappa.matmul(K_total_inducing.T)\n",
    "total_variance =  K_total_total - total_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eb314eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample returns\n",
    "returns_mean_prior = torch.zeros(total_assets_in_history) * returns_mean_a\n",
    "returns_covariance_prior = returns_mean_b * predictive_variance\n",
    "expected_returns_distribution = multivariate_normal(returns_mean_prior, returns_covariance_prior)\n",
    "expected_returns = expected_returns_distribution.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ec5efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate process\n",
    "U_f = inducing_distribution.sample()\n",
    "total_mean = m(kappa,U_f)\n",
    "process_distribution = multivariate_normal(expected_returns+total_mean, returns_covariance_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "852cb815",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sample = process_distribution.sample(sample_shape=torch.Size([number_of_time_steps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6db9a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior training\n",
    "mean_u = torch.Tensor(np.random.random(number_of_inducing_points))\n",
    "mean_u = torch.nn.Parameter(mean_u)\n",
    "\n",
    "Sigma_u = torch.Tensor(np.random.random(number_of_inducing_points))\n",
    "Sigma_u = torch.nn.Parameter(Sigma_u)\n",
    "\n",
    "epsilon_distribution = Normal(torch.zeros_like(mean_u), torch.ones_like(mean_u))\n",
    "posterior_distribution = Normal(mean_u,Sigma_u**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36b3ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reparametrization trick\n",
    "sampled_u = mean_u + Sigma_u*epsilon_distribution.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1af923d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_mean = m(kappa,sampled_u)\n",
    "likelihood = multivariate_normal(expected_returns+sampled_mean, returns_covariance_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a738c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIKELIHOOD = likelihood.log_prob(process_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4a67da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_det = torch.log(posterior_distribution.scale.prod()/torch.det(inducing_distribution.covariance_matrix))\n",
    "trace_ = torch.trace(m(K_inducing_inducing_inverse,torch.diag(Sigma_u)))\n",
    "means_ = m(mean_u,m(K_inducing_inducing_inverse,mean_u))\n",
    "kl = log_det + trace_ + means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcc3e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELBO = LIKELIHOOD.sum() - kl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
